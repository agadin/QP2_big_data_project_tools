{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install tifffile tensorflow scikit-learn matplotlib",
   "id": "8c08d9f83489221f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Define constants\n",
    "NUM_CLASSES = 15  # Number of organ classes\n",
    "\n",
    "# Load and preprocess a single .tif stack without resizing\n",
    "def load_tif_stack(tif_folder):\n",
    "    print(f\"Loading TIFF stack from folder: {tif_folder}\")\n",
    "    tif_files = sorted([os.path.join(tif_folder, f) for f in os.listdir(tif_folder) if f.endswith('.tif')])\n",
    "    stack = []\n",
    "\n",
    "    for tif_file in tif_files:\n",
    "        print(f\"Loading file: {tif_file}\")\n",
    "        image = tiff.imread(tif_file)\n",
    "        stack.append(image)\n",
    "\n",
    "    stack = np.array(stack)\n",
    "    stack = (stack - np.min(stack)) / (np.max(stack) - np.min(stack))  # Normalize\n",
    "    print(f\"Loaded stack shape: {stack.shape}\")\n",
    "    return stack\n",
    "\n",
    "# Autoencoder model for anomaly detection\n",
    "def build_autoencoder(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    # Bottleneck\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    outputs = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Ensure all stacks have consistent dimensions by padding\n",
    "def preprocess_stacks(all_stacks):\n",
    "    # Determine the target shape (largest dimensions across all stacks)\n",
    "    target_shape = tuple(np.max([stack.shape for stack in all_stacks], axis=0))\n",
    "\n",
    "    resized_stacks = []\n",
    "    for stack in all_stacks:\n",
    "        # Calculate padding for each dimension\n",
    "        pad_width = [(0, target_shape[i] - stack.shape[i]) for i in range(len(stack.shape))]\n",
    "        padded_stack = np.pad(stack, pad_width, mode='constant', constant_values=0)  # Pad with zeros\n",
    "        resized_stacks.append(padded_stack)\n",
    "\n",
    "    return np.array(resized_stacks)\n",
    "\n",
    "# Train the autoencoder on slices\n",
    "def train_shared_autoencoder(autoencoder, all_stacks, epochs=5, batch_size=8):\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    all_data = np.concatenate(all_stacks, axis=0)\n",
    "    history = autoencoder.fit(\n",
    "        all_data, all_data,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    return history\n",
    "\n",
    "# Compute reconstruction error for anomaly detection and highlight differences\n",
    "def compute_reconstruction_error(autoencoder, data):\n",
    "    reconstructed = autoencoder.predict(data)\n",
    "    errors = [mean_squared_error(orig.flatten(), recon.flatten()) for orig, recon in zip(data, reconstructed)]\n",
    "    differences = np.abs(data - reconstructed)\n",
    "    return errors, differences\n",
    "\n",
    "# Save slice-by-slice comparisons using Plotly\n",
    "def save_slice_comparisons(differences, stack, reconstructed, output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    for i, (orig, recon, diff) in enumerate(zip(stack, reconstructed, differences)):\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Image(z=orig.squeeze(), name='Original'))\n",
    "        fig.add_trace(go.Image(z=recon.squeeze(), name='Reconstructed'))\n",
    "        fig.add_trace(go.Image(z=diff.squeeze(), name='Difference'))\n",
    "        fig.update_layout(title=f\"Slice {i}\", showlegend=True)\n",
    "        fig.write_html(os.path.join(output_folder, f\"slice_{i:03d}_comparison.html\"))\n",
    "\n",
    "# Generate summary bar chart and boxplots using Plotly\n",
    "def generate_summary_metrics(group_errors, output_path):\n",
    "    group_means = {group: np.mean(errors) for group, errors in group_errors.items()}\n",
    "    group_stds = {group: np.std(errors) for group, errors in group_errors.items()}\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=list(group_means.keys()),\n",
    "        y=list(group_means.values()),\n",
    "        error_y=dict(type='data', array=list(group_stds.values())),\n",
    "        name='Mean Reconstruction Error'\n",
    "    ))\n",
    "\n",
    "    fig.update_layout(title=\"Summary of Reconstruction Errors\",\n",
    "                      xaxis_title=\"Groups\",\n",
    "                      yaxis_title=\"Mean Reconstruction Error\",\n",
    "                      showlegend=True)\n",
    "    fig.write_html(output_path)\n",
    "\n",
    "# Analyze differences between groups using a shared autoencoder\n",
    "def analyze_group_differences_with_shared_autoencoder(group_folders):\n",
    "    all_stacks = []\n",
    "    group_errors = {}\n",
    "\n",
    "    for group in group_folders:\n",
    "        print(f\"Loading data for group: {group}\")\n",
    "        stack = load_tif_stack(group)\n",
    "        stack = stack[..., np.newaxis]  # Add channel dimension\n",
    "        all_stacks.append(stack)\n",
    "\n",
    "    # Preprocess stacks to ensure consistent dimensions\n",
    "    all_stacks = preprocess_stacks(all_stacks)\n",
    "\n",
    "    # Build and train a shared autoencoder\n",
    "    input_shape = all_stacks[0].shape[1:]  # Assume all stacks now have the same shape\n",
    "    autoencoder = build_autoencoder(input_shape)\n",
    "    print(\"Training shared autoencoder on all groups...\")\n",
    "    train_shared_autoencoder(autoencoder, all_stacks)\n",
    "\n",
    "    # Compute reconstruction error for each group\n",
    "    for group, stack in zip(group_folders, all_stacks):\n",
    "        print(f\"Computing reconstruction error for group {group}...\")\n",
    "        errors, differences = compute_reconstruction_error(autoencoder, stack)\n",
    "        group_errors[group] = errors\n",
    "\n",
    "        # Save slice-by-slice comparisons\n",
    "        comparison_folder = os.path.join(group, 'slice_comparisons')\n",
    "        save_slice_comparisons(differences, stack, autoencoder.predict(stack), comparison_folder)\n",
    "\n",
    "    # Generate summary metrics\n",
    "    generate_summary_metrics(group_errors, 'summary_metrics.html')\n",
    "\n",
    "    return group_errors\n",
    "\n",
    "# Example usage\n",
    "# group_folders = [\"/path/to/CT_1\", \"/path/to/CT_2\", \"/path/to/CT_3\", \"/path/to/CT_4\"]\n",
    "# group_differences = analyze_group_differences_with_shared_autoencoder(group_folders)\n",
    "\n",
    "# Example usage\n",
    "group_folders = [\"/Users/alexandergadin/Downloads/group_3-selected/CT_1\", \"/Users/alexandergadin/Downloads/group_3-selected/CT_2\", \"/Users/alexandergadin/Downloads/group_3-selected/CT_3\", \"/Users/alexandergadin/Downloads/group_3-selected/CT_4\"]\n",
    "group_differences = analyze_group_differences_with_shared_autoencoder(group_folders)\n"
   ],
   "id": "364b238a95022523"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b26c39fb5f0b2625"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
